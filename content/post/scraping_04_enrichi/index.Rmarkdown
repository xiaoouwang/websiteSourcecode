---
title: "Scrape lemonde articles for testing nlp libraries (1)"
date: "2020-04-07"
summary:
subtitle:
authors:
- Xiaoou WANG
tags: ["Enrichissement Corpus"]
# categories: [""]
# lastMod: "2019-09-05T00:00:00Z"
featured: false
draft: false
markup: blackfriday
toc: true
# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: ""
  focal_point: ""

---

```{r include=FALSE}
knitr::opts_chunk$set(comment = NA, highlight = TRUE,cache = TRUE)
```

<!-- {{< figure library="true" src="enjoy.png" lightbox="true" >}} -->

{{% toc %}}

## 1. Why

Construct a corpus for testing morpho-syntactic parsers used in Treetagger, Stanford and spacy.

## 2. Results

At the end of this part you will have links for [6489 articles](https://gist.github.com/xiaoouwang/b7fc0c28c72b5b508aaa1dcb34a8b0a6) non classified, 426 articles in culture, 403 articles in sport, 334 articles in economy, all published on the French journal `le monde`.

## 3. Get the links for lemonde's archives

Le monde has some nice archives easily scrapable. The links have the following form:

https://www.lemonde.fr/archives-du-monde/01-02-2020/

So it suffices to use the following python code to generates some dates , one can play around to scrape different range of dates (the code here grabs all the links among 2019/01/01 to 2020/08/29:

```{python}
def getArchiveLinks(daystart, dayend, monthstart, monthend):
    dates = [str(i).zfill(2)+"-"+str(j).zfill(2) +
             "-2019" for i in range(daystart, dayend) for j in range(monthstart, monthend)]
    archive_links = [
        "https://www.lemonde.fr/archives-du-monde/" + date + "/" for date in dates]
    return archive_links
archive_links = getArchiveLinks(1,29,1,9)
print(archive_links[:5])
```

## 4. Filter the links

We use `beautiful soup` and some routine standard python libraries here to handle some exceptions.

The links are embedded in a `teaser class div` so it's easy to grab them. However a lot of articles are subscriber only so we need to filter them. Upon further investigation, exclusive articles have a span element with sr-only as class. So the code to scrape only free-access articles would be:

```{python eval=FALSE}
def getArticlesLinks(archive_links):
    links_non_abonne = []
    for link in archive_links:
        try:
            html = urlopen(link)
        except HTTPError as e:
            print("text url not valid", link)
        soup = BeautifulSoup(html, "html.parser")
        temp = soup.find_all(class_="teaser")
        for item in temp:
            # condition here : if no span sr-only (abonnes)
            if not item.find('span', {'class': 'sr-only'}):
                links_non_abonne.append(item.find('a')['href'])
    return links_non_abonne
```

You can download the file [here](https://gist.github.com/xiaoouwang/b7fc0c28c72b5b508aaa1dcb34a8b0a6)

## 5. Classify the links

It's nice to have 6489 links, but often we need to classify the articles by category to test, for example, some supervised machine learning tasks like text classification.

Let's observe a sport link's form: https://www.lemonde.fr/sport/article/2019/08/28/judo-avec-son-quatrieme-sacre-clarisse-agbegnenou-devient-la-francaise-la-plus-titree-de-l-histoire-des-mondiaux_5503790_3242.html

It's easy to see that it starts with `https://www.lemonde.fr/sport/`. Based on this feature, we can easily classify links according to themes:

```{python eval=FALSE}
def readFile(path):
    with open(path, 'r') as f:
        return f.read()

def classifyLinks(themeList, linkFile):
    dict_links = defaultdict(list)
    for theme in themeList:
        theme_link = 'https://www.lemonde.fr/'+theme+'.*'
        p = re.compile(theme_link)
        theme_links = p.findall(links)
        [dict_links[theme].append(link) for link in theme_links if 'en-direct' not in link]
    return dict_links

links = readFile('lemonde/lemondeLinks.txt')
themes = ['culture', 'sport', 'economie']

dict_links = classifyLinks(themes, links)

for key, value in dict_links.items():
    print(key, len(value))
# culture 426
# sport 403
# economie 334
```

## 6. After

Now it's time to scrape articles' content. This might seems intimidating but it's actually the easist part of all. Time to advance to part 2 !

